{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5erRIgXfE43M",
        "8gmYPxz1FDtl",
        "xP_s9ix3cJx7",
        "YeMI81NdEYmQ",
        "mF1Tc8YKb-Bs",
        "QmTvI-yqcu1R",
        "u8TfaQCLWiM0"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "da1ea01a8812413084e8245f1a6642f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d4a838b02dd4d01b7d892a8452ef77f",
              "IPY_MODEL_762f0a14da694a578ec0075978acff51",
              "IPY_MODEL_44cc8163562245cf8b7f73a17659bd56"
            ],
            "layout": "IPY_MODEL_a2037e151dc04056905cd13261e2c1a1"
          }
        },
        "4d4a838b02dd4d01b7d892a8452ef77f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97591235f9a2499592cb5d2ea389a07e",
            "placeholder": "​",
            "style": "IPY_MODEL_9461eda3c182436c869a34682931ea0a",
            "value": "config.json: 100%"
          }
        },
        "762f0a14da694a578ec0075978acff51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f49d79e409646668a66ec2b729f0a95",
            "max": 1802,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_272cdf648be7435bbd5ef6919ce5ca6b",
            "value": 1802
          }
        },
        "44cc8163562245cf8b7f73a17659bd56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ffb720a42184a0e97aa2270f10d161c",
            "placeholder": "​",
            "style": "IPY_MODEL_214670f5469944b2857c06cabf3a62f8",
            "value": " 1.80k/1.80k [00:00&lt;00:00, 130kB/s]"
          }
        },
        "a2037e151dc04056905cd13261e2c1a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97591235f9a2499592cb5d2ea389a07e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9461eda3c182436c869a34682931ea0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f49d79e409646668a66ec2b729f0a95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "272cdf648be7435bbd5ef6919ce5ca6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ffb720a42184a0e97aa2270f10d161c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "214670f5469944b2857c06cabf3a62f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d33ec049868f406e99c34e1a81988026": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_198b2a90f1364944bebb315cef90aef9",
              "IPY_MODEL_9431de9cce6a45f7a944e1f94373954e",
              "IPY_MODEL_b8c2355fe14041c7b6158aa2fc395591"
            ],
            "layout": "IPY_MODEL_31c19c29004e46c29b6ecd60efe343de"
          }
        },
        "198b2a90f1364944bebb315cef90aef9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_319977fc13394235af10a4a33ccbb104",
            "placeholder": "​",
            "style": "IPY_MODEL_b91a06b97fd74ad592a9b794670ec47b",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "9431de9cce6a45f7a944e1f94373954e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8dea14c393840d7add2d43b1c4b4ce9",
            "max": 1222317369,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07b10d2da9c14fa88f1026f9fa9e7a0a",
            "value": 1222317369
          }
        },
        "b8c2355fe14041c7b6158aa2fc395591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d8a78109ca446108f868ada646614a7",
            "placeholder": "​",
            "style": "IPY_MODEL_2c78a598143a42d490fe9b98e4c95665",
            "value": " 1.22G/1.22G [00:11&lt;00:00, 191MB/s]"
          }
        },
        "31c19c29004e46c29b6ecd60efe343de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "319977fc13394235af10a4a33ccbb104": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b91a06b97fd74ad592a9b794670ec47b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8dea14c393840d7add2d43b1c4b4ce9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07b10d2da9c14fa88f1026f9fa9e7a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d8a78109ca446108f868ada646614a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c78a598143a42d490fe9b98e4c95665": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42c7586651dc4c0c8e6d07946d82626a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92f698a393ec434285ec3bfbf8a03f11",
              "IPY_MODEL_112f7829bb60426786e97e6f737383ab",
              "IPY_MODEL_eb646d871ed346a19c7030cab59c781d"
            ],
            "layout": "IPY_MODEL_59f4a35b477d423491a35d91fad2db4b"
          }
        },
        "92f698a393ec434285ec3bfbf8a03f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6941f4d7320e44ffb62499a57d4614b8",
            "placeholder": "​",
            "style": "IPY_MODEL_c42391361b3549678b794bc1a181276e",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "112f7829bb60426786e97e6f737383ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edf773f3d2da4a1398a7bb38780535e1",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11cc5f48598e4cdbac5b77c72ec9ad9d",
            "value": 26
          }
        },
        "eb646d871ed346a19c7030cab59c781d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45478de6528f4eafa5bf7b18fb911213",
            "placeholder": "​",
            "style": "IPY_MODEL_8c08498782a445b583257d95174d59ad",
            "value": " 26.0/26.0 [00:00&lt;00:00, 1.22kB/s]"
          }
        },
        "59f4a35b477d423491a35d91fad2db4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6941f4d7320e44ffb62499a57d4614b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c42391361b3549678b794bc1a181276e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edf773f3d2da4a1398a7bb38780535e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11cc5f48598e4cdbac5b77c72ec9ad9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45478de6528f4eafa5bf7b18fb911213": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c08498782a445b583257d95174d59ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53024f996a7b48479bdf1a5595aed766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6acc4bd591bd4a2ea807f13a382d99a0",
              "IPY_MODEL_628283585bb44946a6c98d35c6b8392d",
              "IPY_MODEL_62587604ba114e0e9e07df23ee1d4c5f"
            ],
            "layout": "IPY_MODEL_226feadb30764c419c00850ff06ed60c"
          }
        },
        "6acc4bd591bd4a2ea807f13a382d99a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2753faf9297549a6befdbf70e71e1239",
            "placeholder": "​",
            "style": "IPY_MODEL_4209fa6cec264ebc9ebcd733bf916a56",
            "value": "vocab.json: 100%"
          }
        },
        "628283585bb44946a6c98d35c6b8392d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6eac8cd2dc74b4faae0fd74e362594a",
            "max": 898822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ebbfe3366db4ba4b218cac39a73d330",
            "value": 898822
          }
        },
        "62587604ba114e0e9e07df23ee1d4c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f61e898cc7a480f993349eabed7f5da",
            "placeholder": "​",
            "style": "IPY_MODEL_ae09a0a91c9646f788cce2af40969816",
            "value": " 899k/899k [00:00&lt;00:00, 9.91MB/s]"
          }
        },
        "226feadb30764c419c00850ff06ed60c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2753faf9297549a6befdbf70e71e1239": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4209fa6cec264ebc9ebcd733bf916a56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6eac8cd2dc74b4faae0fd74e362594a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ebbfe3366db4ba4b218cac39a73d330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f61e898cc7a480f993349eabed7f5da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae09a0a91c9646f788cce2af40969816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5b46d42b3604cd0a23252a6e62f46ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f98dec3126948138c605800a3d06e90",
              "IPY_MODEL_def150359fdc4931ab003d37175a66dc",
              "IPY_MODEL_044ea673cabf4928a2553f2d14815568"
            ],
            "layout": "IPY_MODEL_76ffadd0d5dc4ee0aaed271016ffbc52"
          }
        },
        "7f98dec3126948138c605800a3d06e90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_641017c8b9924f3ba5c16a0730b5e56f",
            "placeholder": "​",
            "style": "IPY_MODEL_b152c29dbff74446a2979e760c5010e4",
            "value": "merges.txt: 100%"
          }
        },
        "def150359fdc4931ab003d37175a66dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c51a31137c9844b185d0d67dacd70eae",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d95bce6c5ae74b159db843fe9219991c",
            "value": 456318
          }
        },
        "044ea673cabf4928a2553f2d14815568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_301ae4766d1f47fab3f89c3cca34e7bf",
            "placeholder": "​",
            "style": "IPY_MODEL_e058858bce9b441bbaba8d950b41e565",
            "value": " 456k/456k [00:00&lt;00:00, 10.6MB/s]"
          }
        },
        "76ffadd0d5dc4ee0aaed271016ffbc52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "641017c8b9924f3ba5c16a0730b5e56f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b152c29dbff74446a2979e760c5010e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c51a31137c9844b185d0d67dacd70eae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d95bce6c5ae74b159db843fe9219991c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "301ae4766d1f47fab3f89c3cca34e7bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e058858bce9b441bbaba8d950b41e565": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# libraries import"
      ],
      "metadata": {
        "id": "5erRIgXfE43M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install rake_nltk\n",
        "!pip install sentence_transformers\n",
        "!pip install webdriver_manager\n",
        "!pip install python-Levenshtein\n",
        "!pip install spacy\n",
        "!pip install transformers\n",
        "\n",
        "\n",
        "#!pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31Kk_aOBvnyM",
        "outputId": "4ad78a2c-e3cd-41ae-bcfb-a1059e91504b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.44.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.114.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.6)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.6.4)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->gradio) (0.38.5)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: rake_nltk in /usr/local/lib/python3.10/dist-packages (1.0.6)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from rake_nltk) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (4.66.5)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence_transformers) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: webdriver_manager in /usr/local/lib/python3.10/dist-packages (4.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (2.32.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (1.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver_manager) (24.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver_manager) (2024.8.30)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: Levenshtein==0.25.1 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein) (0.25.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.25.1->python-Levenshtein) (3.9.7)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# libraries import"
      ],
      "metadata": {
        "id": "8gmYPxz1FDtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L1WcuNlcWww",
        "outputId": "a0997398-4295-45de-93a8-2d7218ccd923"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start (Try)"
      ],
      "metadata": {
        "id": "xP_s9ix3cJx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "# import nltk\n",
        "# from nltk import word_tokenize\n",
        "# from gensim.models import KeyedVectors\n",
        "# import numpy as np\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# import heapq\n",
        "# import gradio as gr\n",
        "# from collections import Counter\n",
        "\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# # Load pre-trained GloVe embeddings\n",
        "# glove_vectors = KeyedVectors.load_word2vec_format('glove.6B.50d.txt', binary=False, no_header=True)\n",
        "\n",
        "# def scrape_telegraph_india():\n",
        "#     base_url = \"https://www.ndtv.com/\"\n",
        "#     response = requests.get(base_url)\n",
        "#     if response.status_code == 200:\n",
        "#         soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "#         headlines = soup.find_all('h3')\n",
        "#         urls = []\n",
        "#         for h3 in headlines:\n",
        "#             a_tag = h3.find('a')\n",
        "#             if a_tag and 'href' in a_tag.attrs:\n",
        "#                 url = a_tag['href']\n",
        "#                 if url.startswith('/'):\n",
        "#                     url = 'https://www.ndtv.com/' + url.lstrip('/')\n",
        "#                 urls.append((a_tag.get_text(strip=True), url))\n",
        "#         return urls\n",
        "#     else:\n",
        "#         print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "#         return []\n",
        "\n",
        "# def extract_news_from_url(url):\n",
        "#     response = requests.get(url)\n",
        "#     if response.status_code == 200:\n",
        "#         soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "#         content_paragraphs = soup.find_all('p')\n",
        "#         if content_paragraphs:\n",
        "#             content = \"\\n\".join(p.get_text(strip=True) for p in content_paragraphs)\n",
        "#             return content\n",
        "#         return \"Content not found.\"\n",
        "#     else:\n",
        "#         print(f\"Failed to access {url}. Status code: {response.status_code}\")\n",
        "#         return \"Failed to fetch content.\"\n",
        "\n",
        "# class NeuralIRSearchEngine:\n",
        "#     def __init__(self):\n",
        "#         self.news_data, self.news_urls = self.scrape_news()\n",
        "#         self.ngrams = self.create_ngrams(self.news_data)\n",
        "\n",
        "#     def scrape_news(self):\n",
        "#         headlines_and_urls = scrape_telegraph_india()\n",
        "#         news_data = [headline for headline, url in headlines_and_urls]\n",
        "#         news_urls = [url for headline, url in headlines_and_urls]\n",
        "#         return news_data, news_urls\n",
        "\n",
        "#     def create_ngrams(self, data, n=2):\n",
        "#         ngrams = []\n",
        "#         for text in data:\n",
        "#             tokens = word_tokenize(text.lower())\n",
        "#             ngrams.extend(list(nltk.ngrams(tokens, n)))\n",
        "#         return Counter(ngrams)\n",
        "\n",
        "#     def suggest_next_words(self, input_text):\n",
        "#         tokens = word_tokenize(input_text.lower())\n",
        "#         if len(tokens) < 1:\n",
        "#             return []\n",
        "#         last_tokens = tuple(tokens[-1:])  # Single token tuple for bigram or other n-grams\n",
        "#         possible_ngrams = [ngram for ngram in self.ngrams if ngram[:-1] == last_tokens]\n",
        "#         next_word_counts = Counter([ngram[-1] for ngram in possible_ngrams])\n",
        "#         suggestions = [word for word, count in next_word_counts.most_common(5)]\n",
        "#         return suggestions\n",
        "\n",
        "#     def get_average_embedding(self, text):\n",
        "#         words = word_tokenize(text.lower())\n",
        "#         word_embeddings = []\n",
        "#         for word in words:\n",
        "#             if word in glove_vectors:\n",
        "#                 word_embeddings.append(glove_vectors[word])\n",
        "#         if word_embeddings:\n",
        "#             return np.mean(word_embeddings, axis=0)\n",
        "#         else:\n",
        "#             return np.zeros(50)\n",
        "\n",
        "#     def rank_documents(self, query, documents):\n",
        "#         query_embedding = self.get_average_embedding(query)\n",
        "#         doc_embeddings = [self.get_average_embedding(doc) for doc in documents]\n",
        "#         similarities = cosine_similarity([query_embedding], doc_embeddings).flatten()\n",
        "#         ranked_indices = heapq.nlargest(5, range(len(similarities)), similarities.take)\n",
        "#         ranked_documents = [documents[i] for i in ranked_indices]\n",
        "#         return ranked_documents, ranked_indices, similarities\n",
        "\n",
        "#     def generate_summary(self, text):\n",
        "#         # Placeholder for summarization logic, replace with actual implementation\n",
        "#         return f\"Summary of: {text[:100]}...\"\n",
        "\n",
        "#     def search(self, query):\n",
        "#         if query:\n",
        "#             ranked_news, ranked_indices, similarities = self.rank_documents(query, self.news_data)\n",
        "#             summaries = []\n",
        "#             for idx in ranked_indices:\n",
        "#                 full_text = extract_news_from_url(self.news_urls[idx])\n",
        "#                 summary = self.generate_summary(full_text)\n",
        "#                 summaries.append([\n",
        "#                     self.news_data[idx],\n",
        "#                     summary,\n",
        "#                     round(similarities[idx], 2),\n",
        "#                     self.news_urls[idx]  # Adding the URL for redirection\n",
        "#                 ])\n",
        "#             return summaries\n",
        "#         else:\n",
        "#             return \"Please enter a search query.\"\n",
        "\n",
        "# search_engine = NeuralIRSearchEngine()\n",
        "\n",
        "# def gradio_live_recommendations(query):\n",
        "#     return search_engine.suggest_next_words(query)\n",
        "\n",
        "# def gradio_search(query):\n",
        "#     results = search_engine.search(query)\n",
        "#     if isinstance(results, str):\n",
        "#         return results\n",
        "#     # Format results for Dataframe\n",
        "#     formatted_results = []\n",
        "#     for res in results:\n",
        "#         formatted_results.append([\n",
        "#             f'<a href=\"{res[3]}\" target=\"_blank\">{res[0]}</a>',\n",
        "#             res[1],\n",
        "#             res[2]\n",
        "#         ])\n",
        "#     return formatted_results\n",
        "\n",
        "# with gr.Blocks() as demo:\n",
        "#     with gr.Row():\n",
        "#         search_box = gr.Textbox(label=\"Search Query\", placeholder=\"Type here...\", lines=2)\n",
        "#         recommendations_dropdown = gr.Dropdown(label=\"Suggestions\", choices=[], allow_custom_value=True)\n",
        "#         submit_button = gr.Button(\"Search\")\n",
        "\n",
        "#     submit_button.click(gradio_interface, inputs=query_input, outputs=results)\n",
        "\n",
        "#     search_box.change(fn=gradio_live_recommendations, inputs=search_box, outputs=recommendations_dropdown)\n",
        "#     search_box.submit(fn=gradio_search, inputs=search_box, outputs=gr.Dataframe(headers=[\"Headline\", \"Summary\", \"Relevance Score\"], row_count=5))\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     demo.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "id": "eMEfygipvnPg",
        "outputId": "9aead695-df1c-4b3a-fb38-bc2cf9c6de68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://0cdbd3cf7581e39a7e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0cdbd3cf7581e39a7e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 536, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1931, in process_api\n",
            "    inputs = await self.preprocess_data(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1650, in preprocess_data\n",
            "    if block._id in state:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/state_holder.py\", line 104, in __contains__\n",
            "    block = self.blocks_config.blocks[key]\n",
            "KeyError: 71\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://0cdbd3cf7581e39a7e.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "egLWBBrQvb94",
        "outputId": "69e8f8c1-dd6b-48ae-dd46-a82ac42386a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://da470ea1287e48a585.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://da470ea1287e48a585.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 536, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1935, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1520, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 826, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-20-c83835a887de>\", line 122, in gradio_search\n",
            "    results = search_engine.search(query)\n",
            "  File \"<ipython-input-20-c83835a887de>\", line 102, in search\n",
            "    ranked_news, ranked_indices, similarities = self.rank_documents(query, self.news_data)\n",
            "  File \"<ipython-input-20-c83835a887de>\", line 91, in rank_documents\n",
            "    similarities = cosine_similarity([query_embedding], doc_embeddings).flatten()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 214, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/pairwise.py\", line 1578, in cosine_similarity\n",
            "    X, Y = check_pairwise_arrays(X, Y)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/pairwise.py\", line 173, in check_pairwise_arrays\n",
            "    Y = check_array(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 938, in check_array\n",
            "    raise ValueError(\n",
            "ValueError: Expected 2D array, got 1D array instead:\n",
            "array=[].\n",
            "Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://da470ea1287e48a585.gradio.live\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import heapq\n",
        "import gradio as gr\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load pre-trained GloVe embeddings\n",
        "glove_vectors = KeyedVectors.load_word2vec_format('glove.6B.50d.txt', binary=False, no_header=True)\n",
        "\n",
        "def scrape_telegraph_india():\n",
        "    base_url = \"https://www.aljazeera.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        headlines = soup.find_all('h3')\n",
        "        urls = []\n",
        "        for h3 in headlines:\n",
        "            a_tag = h3.find('a')\n",
        "            if a_tag and 'href' in a_tag.attrs:\n",
        "                url = a_tag['href']\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://www.aljazeera.com/' + url.lstrip('/')\n",
        "                urls.append((a_tag.get_text(strip=True), url))\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def extract_news_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        content_paragraphs = soup.find_all('p')\n",
        "        if content_paragraphs:\n",
        "            content = \"\\n\".join(p.get_text(strip=True) for p in content_paragraphs)\n",
        "            return content\n",
        "        return \"Content not found.\"\n",
        "    else:\n",
        "        print(f\"Failed to access {url}. Status code: {response.status_code}\")\n",
        "        return \"Failed to fetch content.\"\n",
        "\n",
        "class NeuralIRSearchEngine:\n",
        "    def __init__(self):\n",
        "        self.news_data, self.news_urls = self.scrape_news()\n",
        "        self.ngrams = self.create_ngrams(self.news_data)\n",
        "\n",
        "    def scrape_news(self):\n",
        "        headlines_and_urls = scrape_telegraph_india()\n",
        "        news_data = [headline for headline, url in headlines_and_urls]\n",
        "        news_urls = [url for headline, url in headlines_and_urls]\n",
        "        return news_data, news_urls\n",
        "\n",
        "    def create_ngrams(self, data, n=2):\n",
        "        ngrams = []\n",
        "        for text in data:\n",
        "            tokens = word_tokenize(text.lower())\n",
        "            ngrams.extend(list(nltk.ngrams(tokens, n)))\n",
        "        return Counter(ngrams)\n",
        "\n",
        "    def suggest_next_words(self, input_text):\n",
        "        tokens = word_tokenize(input_text.lower())\n",
        "        if len(tokens) < 1:\n",
        "            return []\n",
        "        last_tokens = tuple(tokens[-1:])  # Single token tuple for bigram or other n-grams\n",
        "        possible_ngrams = [ngram for ngram in self.ngrams if ngram[:-1] == last_tokens]\n",
        "        next_word_counts = Counter([ngram[-1] for ngram in possible_ngrams])\n",
        "        suggestions = [word for word, count in next_word_counts.most_common(5)]\n",
        "        return suggestions\n",
        "\n",
        "    def get_average_embedding(self, text):\n",
        "        words = word_tokenize(text.lower())\n",
        "        word_embeddings = []\n",
        "        for word in words:\n",
        "            if word in glove_vectors:\n",
        "                word_embeddings.append(glove_vectors[word])\n",
        "        if word_embeddings:\n",
        "            return np.mean(word_embeddings, axis=0)\n",
        "        else:\n",
        "            return np.zeros(50)\n",
        "\n",
        "    def rank_documents(self, query, documents):\n",
        "        query_embedding = self.get_average_embedding(query)\n",
        "        doc_embeddings = [self.get_average_embedding(doc) for doc in documents]\n",
        "        similarities = cosine_similarity([query_embedding], doc_embeddings).flatten()\n",
        "        ranked_indices = heapq.nlargest(5, range(len(similarities)), similarities.take)\n",
        "        ranked_documents = [documents[i] for i in ranked_indices]\n",
        "        return ranked_documents, ranked_indices, similarities\n",
        "\n",
        "    def generate_summary(self, text):\n",
        "        # Placeholder for summarization logic, replace with actual implementation\n",
        "        return f\"Summary of: {text[:100]}...\"\n",
        "\n",
        "    def search(self, query):\n",
        "        if query:\n",
        "            ranked_news, ranked_indices, similarities = self.rank_documents(query, self.news_data)\n",
        "            summaries = []\n",
        "            for idx in ranked_indices:\n",
        "                full_text = extract_news_from_url(self.news_urls[idx])\n",
        "                summary = self.generate_summary(full_text)\n",
        "                summaries.append([\n",
        "                    f'<a href=\"{self.news_urls[idx]}\" target=\"_blank\">{self.news_data[idx]}</a>',\n",
        "                    summary,\n",
        "                    round(similarities[idx], 2)\n",
        "                ])\n",
        "            return summaries\n",
        "        else:\n",
        "            return \"Please enter a search query.\"\n",
        "\n",
        "search_engine = NeuralIRSearchEngine()\n",
        "\n",
        "def gradio_live_recommendations(query):\n",
        "    return search_engine.suggest_next_words(query)\n",
        "\n",
        "def gradio_search(query):\n",
        "    results = search_engine.search(query)\n",
        "    if isinstance(results, str):\n",
        "        return results\n",
        "    return results\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        search_box = gr.Textbox(label=\"Search Query\", placeholder=\"Type here...\", lines=2)\n",
        "        recommendations_dropdown = gr.Dropdown(label=\"Suggestions\", choices=[], allow_custom_value=True)\n",
        "        submit_button = gr.Button(\"Search\")\n",
        "\n",
        "    submit_button.click(fn=gradio_search, inputs=search_box, outputs=gr.Dataframe(headers=[\"Headline\", \"Summary\", \"Relevance Score\"], row_count=5))\n",
        "    search_box.change(fn=gradio_live_recommendations, inputs=search_box, outputs=recommendations_dropdown)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMPILED ALL NEWS\n"
      ],
      "metadata": {
        "id": "YeMI81NdEYmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import heapq\n",
        "import gradio as gr\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load pre-trained GloVe embeddings\n",
        "glove_vectors = KeyedVectors.load_word2vec_format('glove.6B.50d.txt', binary=False, no_header=True)\n",
        "\n",
        "def scrape_telegraph_india():\n",
        "    base_url = \"https://www.thetelegraphindia.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        headlines = soup.find_all('h2')  # Adjust based on actual HTML structure\n",
        "        urls = []\n",
        "        for h2 in headlines:\n",
        "            a_tag = h2.find('a')\n",
        "            if a_tag and 'href' in a_tag.attrs:\n",
        "                url = a_tag['href']\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://www.thetelegraphindia.com/' + url.lstrip('/')\n",
        "                urls.append((a_tag.get_text(strip=True), url))\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def scrape_bbc():\n",
        "    base_url = \"https://www.aljazeera.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        headlines = soup.find_all('h3')\n",
        "        urls = []\n",
        "        for h3 in headlines:\n",
        "            a_tag = h3.find('a')\n",
        "            if a_tag and 'href' in a_tag.attrs:\n",
        "                url = a_tag['href']\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://www.aljazeera.com/' + url.lstrip('/')\n",
        "                urls.append((a_tag.get_text(strip=True), url))\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def scrape_cnn():\n",
        "    base_url = \"https://edition.cnn.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        headlines = soup.find_all('h3')\n",
        "        urls = []\n",
        "        for h3 in headlines:\n",
        "            a_tag = h3.find('a')\n",
        "            if a_tag and 'href' in a_tag.attrs:\n",
        "                url = a_tag['href']\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://edition.cnn.com' + url.lstrip('/')\n",
        "                urls.append((a_tag.get_text(strip=True), url))\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def extract_news_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        content_paragraphs = soup.find_all('p')\n",
        "        if content_paragraphs:\n",
        "            content = \"\\n\".join(p.get_text(strip=True) for p in content_paragraphs)\n",
        "            return content\n",
        "        return \"Content not found.\"\n",
        "    else:\n",
        "        print(f\"Failed to access {url}. Status code: {response.status_code}\")\n",
        "        return \"Failed to fetch content.\"\n",
        "\n",
        "class NeuralIRSearchEngine:\n",
        "    def __init__(self):\n",
        "        self.news_data, self.news_urls = self.scrape_news()\n",
        "        self.ngrams = self.create_ngrams(self.news_data)\n",
        "\n",
        "    def scrape_news(self):\n",
        "        telegraph_data = scrape_telegraph_india()\n",
        "        bbc_data = scrape_bbc()\n",
        "        cnn_data = scrape_cnn()\n",
        "\n",
        "        all_data = telegraph_data + bbc_data + cnn_data\n",
        "        news_data = [headline for headline, url in all_data]\n",
        "        news_urls = [url for headline, url in all_data]\n",
        "        return news_data, news_urls\n",
        "\n",
        "    def create_ngrams(self, data, n=2):\n",
        "        ngrams = []\n",
        "        for text in data:\n",
        "            tokens = word_tokenize(text.lower())\n",
        "            ngrams.extend(list(nltk.ngrams(tokens, n)))\n",
        "        return Counter(ngrams)\n",
        "\n",
        "    def suggest_next_words(self, input_text):\n",
        "        tokens = word_tokenize(input_text.lower())\n",
        "        if len(tokens) < 1:\n",
        "            return []\n",
        "        last_tokens = tuple(tokens[-1:])  # Single token tuple for bigram or other n-grams\n",
        "        possible_ngrams = [ngram for ngram in self.ngrams if ngram[:-1] == last_tokens]\n",
        "        next_word_counts = Counter([ngram[-1] for ngram in possible_ngrams])\n",
        "        suggestions = [word for word, count in next_word_counts.most_common(5)]\n",
        "        return suggestions\n",
        "\n",
        "    def get_average_embedding(self, text):\n",
        "        words = word_tokenize(text.lower())\n",
        "        word_embeddings = []\n",
        "        for word in words:\n",
        "            if word in glove_vectors:\n",
        "                word_embeddings.append(glove_vectors[word])\n",
        "        if word_embeddings:\n",
        "            return np.mean(word_embeddings, axis=0)\n",
        "        else:\n",
        "            return np.zeros(50)\n",
        "\n",
        "    def rank_documents(self, query, documents):\n",
        "        query_embedding = self.get_average_embedding(query)\n",
        "\n",
        "        # Check if query_embedding is valid\n",
        "        if np.all(query_embedding == 0):\n",
        "            return [], [], []\n",
        "\n",
        "        doc_embeddings = [self.get_average_embedding(doc) for doc in documents]\n",
        "\n",
        "        # Check if doc_embeddings contains valid data\n",
        "        if not doc_embeddings:\n",
        "            return [], [], []\n",
        "\n",
        "        # Convert doc_embeddings to a 2D array for cosine_similarity\n",
        "        doc_embeddings = np.array(doc_embeddings)\n",
        "\n",
        "        similarities = cosine_similarity([query_embedding], doc_embeddings).flatten()\n",
        "        ranked_indices = heapq.nlargest(5, range(len(similarities)), similarities.take)\n",
        "        ranked_documents = [documents[i] for i in ranked_indices]\n",
        "\n",
        "        return ranked_documents, ranked_indices, similarities\n",
        "\n",
        "    def generate_summary(self, text):\n",
        "        # Placeholder for summarization logic, replace with actual implementation\n",
        "        return f\"Summary of: {text[:100]}...\"\n",
        "\n",
        "    def search(self, query):\n",
        "        if query:\n",
        "            ranked_news, ranked_indices, similarities = self.rank_documents(query, self.news_data)\n",
        "            summaries = []\n",
        "            for idx in ranked_indices:\n",
        "                full_text = extract_news_from_url(self.news_urls[idx])\n",
        "                summary = self.generate_summary(full_text)\n",
        "                summaries.append([\n",
        "                    f'<a href=\"{self.news_urls[idx]}\" target=\"_blank\">{self.news_data[idx]}</a>',\n",
        "                    summary,\n",
        "                    round(similarities[idx], 2)\n",
        "                ])\n",
        "            return summaries\n",
        "        else:\n",
        "            return \"Please enter a search query.\"\n",
        "\n",
        "search_engine = NeuralIRSearchEngine()\n",
        "\n",
        "def gradio_live_recommendations(query):\n",
        "    return search_engine.suggest_next_words(query)\n",
        "\n",
        "def gradio_search(query):\n",
        "    results = search_engine.search(query)\n",
        "    if isinstance(results, str):\n",
        "        return results\n",
        "    return results\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        search_box = gr.Textbox(label=\"Search Query\", placeholder=\"Type here...\", lines=2)\n",
        "        recommendations_dropdown = gr.Dropdown(label=\"Suggestions\", choices=[], allow_custom_value=True)\n",
        "        submit_button = gr.Button(\"Search\")\n",
        "\n",
        "    submit_button.click(fn=gradio_search, inputs=search_box, outputs=gr.Dataframe(headers=[\"Headline\", \"Summary\", \"Relevance Score\"], row_count=5))\n",
        "    search_box.change(fn=gradio_live_recommendations, inputs=search_box, outputs=recommendations_dropdown)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "yW90M4krBTV3",
        "outputId": "a4efe24d-8583-421e-ba6c-eb36f0618c19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://85d449090eb247797e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://85d449090eb247797e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://85d449090eb247797e.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Added spellcheck"
      ],
      "metadata": {
        "id": "mF1Tc8YKb-Bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import heapq\n",
        "import gradio as gr\n",
        "from collections import Counter\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load pre-trained GloVe embeddings\n",
        "glove_vectors = KeyedVectors.load_word2vec_format('glove.6B.50d.txt', binary=False, no_header=True)\n",
        "\n",
        "def scrape_telegraph_india():\n",
        "    base_url = \"https://www.thetelegraphindia.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        headlines = soup.find_all('h2')\n",
        "        urls = []\n",
        "        for h2 in headlines:\n",
        "            a_tag = h2.find('a')\n",
        "            if a_tag and 'href' in a_tag.attrs:\n",
        "                url = a_tag['href']\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://www.thetelegraphindia.com/' + url.lstrip('/')\n",
        "                urls.append((a_tag.get_text(strip=True), url))\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def scrape_bbc():\n",
        "    base_url = \"https://www.aljazeera.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        headlines = soup.find_all('h3')\n",
        "        urls = []\n",
        "        for h3 in headlines:\n",
        "            a_tag = h3.find('a')\n",
        "            if a_tag and 'href' in a_tag.attrs:\n",
        "                url = a_tag['href']\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://www.aljazeera.com/' + url.lstrip('/')\n",
        "                urls.append((a_tag.get_text(strip=True), url))\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def scrape_cnn():\n",
        "    base_url = \"https://edition.cnn.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        headlines = soup.find_all('h3')\n",
        "        urls = []\n",
        "        for h3 in headlines:\n",
        "            a_tag = h3.find('a')\n",
        "            if a_tag and 'href' in a_tag.attrs:\n",
        "                url = a_tag['href']\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://edition.cnn.com' + url.lstrip('/')\n",
        "                urls.append((a_tag.get_text(strip=True), url))\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def extract_news_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        content_paragraphs = soup.find_all('p')\n",
        "        if content_paragraphs:\n",
        "            content = \"\\n\".join(p.get_text(strip=True) for p in content_paragraphs)\n",
        "            return content\n",
        "        return \"Content not found.\"\n",
        "    else:\n",
        "        print(f\"Failed to access {url}. Status code: {response.status_code}\")\n",
        "        return \"Failed to fetch content.\"\n",
        "\n",
        "class NeuralIRSearchEngine:\n",
        "    def __init__(self):\n",
        "        self.news_data, self.news_urls = self.scrape_news()\n",
        "        self.ngrams = self.create_ngrams(self.news_data)\n",
        "        self.corpus_words = self.create_corpus_words(self.news_data)\n",
        "\n",
        "    def scrape_news(self):\n",
        "        telegraph_data = scrape_telegraph_india()\n",
        "        bbc_data = scrape_bbc()\n",
        "        cnn_data = scrape_cnn()\n",
        "\n",
        "        all_data = telegraph_data + bbc_data + cnn_data\n",
        "        news_data = [headline for headline, url in all_data]\n",
        "        news_urls = [url for headline, url in all_data]\n",
        "        return news_data, news_urls\n",
        "\n",
        "    def create_ngrams(self, data, n=2):\n",
        "        ngrams = []\n",
        "        for text in data:\n",
        "            tokens = word_tokenize(text.lower())\n",
        "            ngrams.extend(list(nltk.ngrams(tokens, n)))\n",
        "        return Counter(ngrams)\n",
        "\n",
        "    def create_corpus_words(self, data):\n",
        "        words = []\n",
        "        for text in data:\n",
        "            tokens = word_tokenize(text.lower())\n",
        "            words.extend(tokens)\n",
        "        return set(words)\n",
        "\n",
        "    def suggest_next_words(self, input_text):\n",
        "        tokens = word_tokenize(input_text.lower())\n",
        "        if len(tokens) < 1:\n",
        "            return []\n",
        "        last_tokens = tuple(tokens[-1:])\n",
        "        possible_ngrams = [ngram for ngram in self.ngrams if ngram[:-1] == last_tokens]\n",
        "        next_word_counts = Counter([ngram[-1] for ngram in possible_ngrams])\n",
        "        suggestions = [word for word, count in next_word_counts.most_common(5)]\n",
        "        return suggestions\n",
        "\n",
        "    def get_average_embedding(self, text):\n",
        "        words = word_tokenize(text.lower())\n",
        "        word_embeddings = []\n",
        "        for word in words:\n",
        "            if word in glove_vectors:\n",
        "                word_embeddings.append(glove_vectors[word])\n",
        "        if word_embeddings:\n",
        "            return np.mean(word_embeddings, axis=0)\n",
        "        else:\n",
        "            return np.zeros(50)\n",
        "\n",
        "    def rank_documents(self, query, documents):\n",
        "        query_embedding = self.get_average_embedding(query)\n",
        "\n",
        "        if np.all(query_embedding == 0):\n",
        "            return [], [], []\n",
        "\n",
        "        doc_embeddings = [self.get_average_embedding(doc) for doc in documents]\n",
        "\n",
        "        if not doc_embeddings:\n",
        "            return [], [], []\n",
        "\n",
        "        doc_embeddings = np.array(doc_embeddings)\n",
        "\n",
        "        similarities = cosine_similarity([query_embedding], doc_embeddings).flatten()\n",
        "        ranked_indices = heapq.nlargest(5, range(len(similarities)), similarities.take)\n",
        "        ranked_documents = [documents[i] for i in ranked_indices]\n",
        "\n",
        "        return ranked_documents, ranked_indices, similarities\n",
        "\n",
        "    def generate_summary(self, text):\n",
        "        return f\"Summary of: {text[:100]}...\"\n",
        "\n",
        "    def search(self, query):\n",
        "        if query:\n",
        "            ranked_news, ranked_indices, similarities = self.rank_documents(query, self.news_data)\n",
        "            summaries = []\n",
        "            for idx in ranked_indices:\n",
        "                full_text = extract_news_from_url(self.news_urls[idx])\n",
        "                summary = self.generate_summary(full_text)\n",
        "                summaries.append([\n",
        "                    f'{self.news_data[idx]}',\n",
        "                    summary,\n",
        "                    round(similarities[idx], 2)\n",
        "                ])\n",
        "            return summaries\n",
        "        else:\n",
        "            return \"Please enter a search query.\"\n",
        "\n",
        "    def correct_spelling(self, input_text):\n",
        "        tokens = word_tokenize(input_text.lower())\n",
        "        corrected_tokens = []\n",
        "        for token in tokens:\n",
        "            if token not in self.corpus_words:\n",
        "                # Find closest match using Levenshtein distance\n",
        "                closest_match = min(self.corpus_words, key=lambda word: levenshtein_distance(token, word))\n",
        "                corrected_tokens.append(closest_match)\n",
        "            else:\n",
        "                corrected_tokens.append(token)\n",
        "        return ' '.join(corrected_tokens)\n",
        "\n",
        "search_engine = NeuralIRSearchEngine()\n",
        "\n",
        "\n",
        "def gradio_search(query):\n",
        "    results = search_engine.search(query)\n",
        "    if isinstance(results, str):\n",
        "        return results\n",
        "    return results\n",
        "\n",
        "def gradio_live_recommendations(query):\n",
        "    corrected_query = search_engine.correct_spelling(query)\n",
        "    suggestions = search_engine.suggest_next_words(corrected_query)\n",
        "    if corrected_query.lower() != query.lower():\n",
        "        return f\"Did you mean '{corrected_query}'?\", suggestions\n",
        "    else:\n",
        "        return \"\", suggestions\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        search_box = gr.Textbox(label=\"Search Query\", placeholder=\"Type here...\", lines=2)\n",
        "        recommendations_dropdown = gr.Dropdown(label=\"Suggestions\", choices=[], allow_custom_value=True)\n",
        "        correction_message = gr.Textbox(label=\"\", placeholder=\"Go on and ask your query\", lines=1)\n",
        "        submit_button = gr.Button(\"Search\")\n",
        "\n",
        "    submit_button.click(fn=gradio_search, inputs=search_box, outputs=gr.Dataframe(headers=[\"Headline\", \"Summary\", \"Relevance Score\"], row_count=5))\n",
        "    search_box.change(fn=gradio_live_recommendations, inputs=search_box, outputs=[correction_message, recommendations_dropdown])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "vkL39BTnCl-3",
        "outputId": "0a72e0bf-8008-45d3-b8c7-c6f6182aae81"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://712f34623ad72f7b56.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://712f34623ad72f7b56.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://712f34623ad72f7b56.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying NER for emphasis"
      ],
      "metadata": {
        "id": "QmTvI-yqcu1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.chunk import tree2conlltags\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import heapq\n",
        "import gradio as gr\n",
        "from collections import Counter\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# Load pre-trained GloVe embeddings\n",
        "glove_vectors = KeyedVectors.load_word2vec_format('glove.6B.50d.txt', binary=False, no_header=True)\n",
        "\n",
        "def scrape_telegraph_india():\n",
        "    base_url = \"https://www.thetelegraphindia.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        headlines = soup.find_all('h2')\n",
        "        urls = []\n",
        "        for h2 in headlines:\n",
        "            a_tag = h2.find('a')\n",
        "            if a_tag and 'href' in a_tag.attrs:\n",
        "                url = a_tag['href']\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://www.thetelegraphindia.com/' + url.lstrip('/')\n",
        "                urls.append((a_tag.get_text(strip=True), url))\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def scrape_bbc():\n",
        "    base_url = \"https://www.aljazeera.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        headlines = soup.find_all('h3')\n",
        "        urls = []\n",
        "        for h3 in headlines:\n",
        "            a_tag = h3.find('a')\n",
        "            if a_tag and 'href' in a_tag.attrs:\n",
        "                url = a_tag['href']\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://www.aljazeera.com/' + url.lstrip('/')\n",
        "                urls.append((a_tag.get_text(strip=True), url))\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def scrape_cnn():\n",
        "    base_url = \"https://edition.cnn.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        headlines = soup.find_all('h3')\n",
        "        urls = []\n",
        "        for h3 in headlines:\n",
        "            a_tag = h3.find('a')\n",
        "            if a_tag and 'href' in a_tag.attrs:\n",
        "                url = a_tag['href']\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://edition.cnn.com' + url.lstrip('/')\n",
        "                urls.append((a_tag.get_text(strip=True), url))\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def extract_news_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        content_paragraphs = soup.find_all('p')\n",
        "        if content_paragraphs:\n",
        "            content = \"\\n\".join(p.get_text(strip=True) for p in content_paragraphs)\n",
        "            return content\n",
        "        return \"Content not found.\"\n",
        "    else:\n",
        "        print(f\"Failed to access {url}. Status code: {response.status_code}\")\n",
        "        return \"Failed to fetch content.\"\n",
        "\n",
        "def extract_entities(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    chunked = ne_chunk(pos_tags, binary=False)\n",
        "    iob_tagged = tree2conlltags(chunked)\n",
        "    entities = [word for word, pos, ner in iob_tagged if ner != 'O']\n",
        "    return entities\n",
        "\n",
        "class NeuralIRSearchEngine:\n",
        "    def __init__(self):\n",
        "        self.news_data, self.news_urls = self.scrape_news()\n",
        "        self.ngrams = self.create_ngrams(self.news_data)\n",
        "        self.corpus_words = self.create_corpus_words(self.news_data)\n",
        "        self.entities = [extract_entities(headline) for headline in self.news_data]\n",
        "\n",
        "    def scrape_news(self):\n",
        "        telegraph_data = scrape_telegraph_india()\n",
        "        bbc_data = scrape_bbc()\n",
        "        cnn_data = scrape_cnn()\n",
        "\n",
        "        all_data = telegraph_data + bbc_data + cnn_data\n",
        "        news_data = [headline for headline, url in all_data]\n",
        "        news_urls = [url for headline, url in all_data]\n",
        "        return news_data, news_urls\n",
        "\n",
        "    def create_ngrams(self, data, n=2):\n",
        "        ngrams = []\n",
        "        for text in data:\n",
        "            tokens = word_tokenize(text.lower())\n",
        "            ngrams.extend(list(nltk.ngrams(tokens, n)))\n",
        "        return Counter(ngrams)\n",
        "\n",
        "    def create_corpus_words(self, data):\n",
        "        words = []\n",
        "        for text in data:\n",
        "            tokens = word_tokenize(text.lower())\n",
        "            words.extend(tokens)\n",
        "        return set(words)\n",
        "\n",
        "    def suggest_next_words(self, input_text):\n",
        "        tokens = word_tokenize(input_text.lower())\n",
        "        if len(tokens) < 1:\n",
        "            return []\n",
        "        last_tokens = tuple(tokens[-1:])\n",
        "        possible_ngrams = [ngram for ngram in self.ngrams if ngram[:-1] == last_tokens]\n",
        "        next_word_counts = Counter([ngram[-1] for ngram in possible_ngrams])\n",
        "        suggestions = [word for word, count in next_word_counts.most_common(5)]\n",
        "        return suggestions\n",
        "\n",
        "    def get_average_embedding(self, text):\n",
        "        words = word_tokenize(text.lower())\n",
        "        word_embeddings = []\n",
        "        for word in words:\n",
        "            if word in glove_vectors:\n",
        "                word_embeddings.append(glove_vectors[word])\n",
        "        if word_embeddings:\n",
        "            return np.mean(word_embeddings, axis=0)\n",
        "        else:\n",
        "            return np.zeros(50)\n",
        "\n",
        "    def rank_documents(self, query, documents):\n",
        "        query_embedding = self.get_average_embedding(query)\n",
        "\n",
        "        if np.all(query_embedding == 0):\n",
        "            return [], [], []\n",
        "\n",
        "        doc_embeddings = [self.get_average_embedding(doc) for doc in documents]\n",
        "\n",
        "        if not doc_embeddings:\n",
        "            return [], [], []\n",
        "\n",
        "        doc_embeddings = np.array(doc_embeddings)\n",
        "\n",
        "        similarities = cosine_similarity([query_embedding], doc_embeddings).flatten()\n",
        "        ranked_indices = heapq.nlargest(5, range(len(similarities)), similarities.take)\n",
        "        ranked_documents = [documents[i] for i in ranked_indices]\n",
        "\n",
        "        return ranked_documents, ranked_indices, similarities\n",
        "\n",
        "    def generate_summary(self, text):\n",
        "        return f\"Summary of: {text[:100]}...\"\n",
        "\n",
        "    def search(self, query):\n",
        "        if query:\n",
        "            ranked_news, ranked_indices, similarities = self.rank_documents(query, self.news_data)\n",
        "            summaries = []\n",
        "            for idx in ranked_indices:\n",
        "                full_text = extract_news_from_url(self.news_urls[idx])\n",
        "                summary = self.generate_summary(full_text)\n",
        "                summaries.append([\n",
        "                    f'{self.news_data[idx]}',\n",
        "                    summary,\n",
        "                    round(similarities[idx], 2),\n",
        "                    \", \".join(self.entities[idx])\n",
        "                ])\n",
        "            return summaries\n",
        "        else:\n",
        "            return \"Please enter a search query.\"\n",
        "\n",
        "    def correct_spelling(self, input_text):\n",
        "        tokens = word_tokenize(input_text.lower())\n",
        "        corrected_tokens = []\n",
        "        for token in tokens:\n",
        "            if token not in self.corpus_words:\n",
        "                # Find closest match using Levenshtein distance\n",
        "                closest_match = min(self.corpus_words, key=lambda word: levenshtein_distance(token, word))\n",
        "                corrected_tokens.append(closest_match)\n",
        "            else:\n",
        "                corrected_tokens.append(token)\n",
        "        return ' '.join(corrected_tokens)\n",
        "\n",
        "search_engine = NeuralIRSearchEngine()\n",
        "\n",
        "def gradio_search(query):\n",
        "    results = search_engine.search(query)\n",
        "    if isinstance(results, str):\n",
        "        return results\n",
        "    return results\n",
        "\n",
        "def gradio_live_recommendations(query):\n",
        "    corrected_query = search_engine.correct_spelling(query)\n",
        "    suggestions = search_engine.suggest_next_words(corrected_query)\n",
        "    if corrected_query.lower() != query.lower():\n",
        "        return f\"Did you mean '{corrected_query}'?\", suggestions\n",
        "    else:\n",
        "        return \"\", suggestions\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        search_box = gr.Textbox(label=\"Search Query\", placeholder=\"Type here...\", lines=2)\n",
        "        recommendations_dropdown = gr.Dropdown(label=\"Suggestions\", choices=[], allow_custom_value=True)\n",
        "        correction_message = gr.Textbox(label=\"\", placeholder=\"Go on and ask your query\", lines=1)\n",
        "        submit_button = gr.Button(\"Search\")\n",
        "\n",
        "    submit_button.click(fn=gradio_search, inputs=search_box, outputs=gr.Dataframe(headers=[\"Headline\", \"Summary\", \"Relevance Score\", \"Keywords\"], row_count=5))\n",
        "    search_box.change(fn=gradio_live_recommendations, inputs=search_box, outputs=[correction_message, recommendations_dropdown])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "6KPWv3FLE3Fp",
        "outputId": "097de085-2040-43ea-a613-e94c3248fa0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://e5e208076e0112c9a8.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e5e208076e0112c9a8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://e5e208076e0112c9a8.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Rake Algorithm"
      ],
      "metadata": {
        "id": "Y3sLrQZNtELE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from transformers import pipeline\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.chunk import tree2conlltags\n",
        "from rake_nltk import Rake\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import heapq\n",
        "import gradio as gr\n",
        "from collections import Counter\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "custom_css = \"\"\"\n",
        ".gradio-container {\n",
        "    background: url(https://res.cloudinary.com/dye74kvqk/image/upload/v1726165064/5b5f024f-39cd-4c91-8973-8515968b68d9.png);\n",
        "    background-size: cover;\n",
        "    background-position: center;\n",
        "}\n",
        "\n",
        ".gradio-container h1 {\n",
        "    text-align: center;\n",
        "    color: white;\n",
        "    font-size: 50px;\n",
        "    margin-left:70%\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Load pre-trained GloVe embeddings\n",
        "glove_vectors = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/nlp/glove.6B.50d.txt', binary=False, no_header=True)\n",
        "\n",
        "#Scraping process begins - we can add more websites to increase the reach\n",
        "def scrape_telegraph_india():\n",
        "    base_url = \"https://www.ndtv.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        headlines = soup.find_all('h2')\n",
        "        urls = []\n",
        "        for h2 in headlines:\n",
        "            a_tag = h2.find('a')\n",
        "            if a_tag and 'href' in a_tag.attrs:\n",
        "                url = a_tag['href']\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://www.ndtv.com/' + url.lstrip('/')\n",
        "                urls.append((a_tag.get_text(strip=True), url))\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def scrape_bbc():\n",
        "    base_url = \"https://www.aljazeera.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        headlines = soup.find_all('h3')\n",
        "        urls = []\n",
        "        for h3 in headlines:\n",
        "            a_tag = h3.find('a')\n",
        "            if a_tag and 'href' in a_tag.attrs:\n",
        "                url = a_tag['href']\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://www.aljazeera.com/' + url.lstrip('/')\n",
        "                urls.append((a_tag.get_text(strip=True), url))\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def scrape_cnn():\n",
        "    base_url = \"https://www.cnn.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        # Find all span elements with the specified class\n",
        "        headline_spans = soup.find_all('span', class_='container__headline-text')\n",
        "        urls = []\n",
        "        for span in headline_spans:\n",
        "            headline_text = span.get_text(strip=True)\n",
        "            # Find the parent <a> tag for the URL\n",
        "            parent_a_tag = span.find_parent('a')\n",
        "            if parent_a_tag and 'href' in parent_a_tag.attrs:\n",
        "                url = parent_a_tag['href']\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://www.cnn.com/' + url.lstrip('/')\n",
        "                urls.append((headline_text, url))\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "# Here we are extracting the content from the news.\n",
        "def extract_news_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        content_paragraphs = soup.find_all('p')\n",
        "        if content_paragraphs:\n",
        "            content = \"\\n\".join(p.get_text(strip=True) for p in content_paragraphs)\n",
        "            return content\n",
        "        return \"Content not found.\"\n",
        "    else:\n",
        "        print(f\"Failed to access {url}. Status code: {response.status_code}\")\n",
        "        return \"Failed to fetch content.\"\n",
        "\n",
        "# Now extract places names organisations to highlight them in the headlines\n",
        "def extract_entities(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    chunked = ne_chunk(pos_tags, binary=False)\n",
        "    iob_tagged = tree2conlltags(chunked)\n",
        "    entities = [word for word, pos, ner in iob_tagged if ner != 'O']\n",
        "    return entities\n",
        "\n",
        "# Using Rake to get keywords\n",
        "def extract_keywords(text):\n",
        "    r = Rake()  # Initialize RAKE\n",
        "    r.extract_keywords_from_text(text)\n",
        "    all_keywords = r.get_ranked_phrases()\n",
        "\n",
        "    filtered_keywords = [kw for kw in all_keywords if len(kw.split()) <= 2]\n",
        "\n",
        "    top_keywords = filtered_keywords[:10]\n",
        "\n",
        "    return ', '.join(top_keywords)\n",
        "\n",
        "# Main code begins  where we implement information retrieval\n",
        "class NeuralIRSearchEngine:\n",
        "    def __init__(self):\n",
        "        self.news_data, self.news_urls = self.scrape_news()\n",
        "        self.ngrams = self.create_ngrams(self.news_data)\n",
        "        self.corpus_words = self.create_corpus_words(self.news_data)\n",
        "        self.entities = [extract_entities(headline) for headline in self.news_data]\n",
        "        self.summarizer = pipeline(\"summarization\")\n",
        "\n",
        "    def scrape_news(self):\n",
        "        telegraph_data = scrape_telegraph_india()\n",
        "        bbc_data = scrape_bbc()\n",
        "        cnn_data = scrape_cnn()\n",
        "\n",
        "        all_data = telegraph_data + bbc_data + cnn_data\n",
        "        news_data = [headline for headline, url in all_data]\n",
        "        news_urls = [url for headline, url in all_data]\n",
        "        return news_data, news_urls\n",
        "\n",
        "    def create_ngrams(self, data, n=2):\n",
        "        ngrams = []\n",
        "        for text in data:\n",
        "            tokens = word_tokenize(text.lower())\n",
        "            ngrams.extend(list(nltk.ngrams(tokens, n)))\n",
        "        return Counter(ngrams)\n",
        "\n",
        "    def create_corpus_words(self, data):\n",
        "        words = []\n",
        "        for text in data:\n",
        "            tokens = word_tokenize(text.lower())\n",
        "            words.extend(tokens)\n",
        "        return set(words)\n",
        "\n",
        "    def suggest_next_words(self, input_text):\n",
        "        tokens = word_tokenize(input_text.lower())\n",
        "        if len(tokens) < 1:\n",
        "            return []\n",
        "        last_tokens = tuple(tokens[-1:])\n",
        "        possible_ngrams = [ngram for ngram in self.ngrams if ngram[:-1] == last_tokens]\n",
        "        next_word_counts = Counter([ngram[-1] for ngram in possible_ngrams])\n",
        "        suggestions = [word for word, count in next_word_counts.most_common(5)]\n",
        "        return suggestions\n",
        "\n",
        "    def get_average_embedding(self, text):\n",
        "        words = word_tokenize(text.lower())\n",
        "        word_embeddings = []\n",
        "        for word in words:\n",
        "            if word in glove_vectors:\n",
        "                word_embeddings.append(glove_vectors[word])\n",
        "        if word_embeddings:\n",
        "            return np.mean(word_embeddings, axis=0)\n",
        "        else:\n",
        "            return np.zeros(50)\n",
        "\n",
        "    def rank_documents(self, query, documents):\n",
        "        query_embedding = self.get_average_embedding(query)\n",
        "\n",
        "        if np.all(query_embedding == 0):\n",
        "            return [], [], []\n",
        "\n",
        "        doc_embeddings = [self.get_average_embedding(doc) for doc in documents]\n",
        "\n",
        "        if not doc_embeddings:\n",
        "            return [], [], []\n",
        "\n",
        "        doc_embeddings = np.array(doc_embeddings)\n",
        "\n",
        "        similarities = cosine_similarity([query_embedding], doc_embeddings).flatten()\n",
        "        ranked_indices = heapq.nlargest(5, range(len(similarities)), similarities.take)\n",
        "        ranked_documents = [documents[i] for i in ranked_indices]\n",
        "\n",
        "        return ranked_documents, ranked_indices, similarities\n",
        "\n",
        "    def generate_summary(self, text):\n",
        "        # return f\"{text[:100]}...\"\n",
        "        try:\n",
        "            max_input_length = 1024\n",
        "            input_tokens = self.summarizer.tokenizer(text, return_tensors='pt', truncation=True, max_length=max_input_length)\n",
        "            input_text = self.summarizer.tokenizer.decode(input_tokens['input_ids'][0], skip_special_tokens=True)\n",
        "\n",
        "            summary = self.summarizer(input_text, max_length=150, min_length=50, do_sample=False)[0]['summary_text']\n",
        "            return summary\n",
        "        except Exception as e:\n",
        "            print(f\"Error during summarization: {e}\")\n",
        "            return \"Summary could not be generated.\"\n",
        "\n",
        "    def search(self, query):\n",
        "        if query:\n",
        "            ranked_news, ranked_indices, similarities = self.rank_documents(query, self.news_data)\n",
        "            summaries = []\n",
        "            for idx in ranked_indices:\n",
        "                full_text = extract_news_from_url(self.news_urls[idx])\n",
        "                summary = self.generate_summary(full_text)\n",
        "                keywords = extract_keywords(full_text)\n",
        "                summaries.append([\n",
        "                    f'{self.news_data[idx]}',\n",
        "                    summary,\n",
        "                    round(similarities[idx], 2),\n",
        "                    keywords,\n",
        "                    \"-- \".join(self.entities[idx]),\n",
        "                    self.news_urls[idx]  # Adding the URL to the output\n",
        "                ])\n",
        "            return summaries\n",
        "        else:\n",
        "            return \"Please enter a search query.\"\n",
        "\n",
        "    def correct_spelling(self, input_text):\n",
        "        tokens = word_tokenize(input_text.lower())\n",
        "        corrected_tokens = []\n",
        "        for token in tokens:\n",
        "            if token not in self.corpus_words:\n",
        "                # Find closest match using Levenshtein distance\n",
        "                closest_match = min(self.corpus_words, key=lambda word: levenshtein_distance(token, word))\n",
        "                corrected_tokens.append(closest_match)\n",
        "            else:\n",
        "                corrected_tokens.append(token)\n",
        "        return ' '.join(corrected_tokens)\n",
        "\n",
        "search_engine = NeuralIRSearchEngine()\n",
        "\n",
        "def gradio_search(query):\n",
        "    results = search_engine.search(query)\n",
        "    if isinstance(results, str):\n",
        "        return results\n",
        "    return results\n",
        "\n",
        "def gradio_live_recommendations(query):\n",
        "    corrected_query = search_engine.correct_spelling(query)\n",
        "    suggestions = search_engine.suggest_next_words(corrected_query)\n",
        "    if corrected_query.lower() != query.lower():\n",
        "        return f\"Did you mean '{corrected_query}'?\", suggestions\n",
        "    else:\n",
        "        return \"\", suggestions\n",
        "\n",
        "title_html = \"<h1>Super Nova</h1>\"\n",
        "\n",
        "with gr.Blocks(css=custom_css) as demo:\n",
        "    gr.HTML(title_html)\n",
        "\n",
        "    with gr.Row():\n",
        "        search_box = gr.Textbox(label=\"Search Query\", placeholder=\"Type here...\", lines=2)\n",
        "        recommendations_dropdown = gr.Dropdown(label=\"Suggestions\", choices=[], allow_custom_value=True)\n",
        "        correction_message = gr.Textbox(label=\"\", placeholder=\"Go on and ask your query\", lines=1)\n",
        "        submit_button = gr.Button(\"Search\")\n",
        "\n",
        "    submit_button.click(fn=gradio_search, inputs=search_box, outputs=gr.Dataframe(headers=[\"Headline\", \"Summary\", \"Relevance Score\", \"Highlights\", \"Keywords\", \"URL\"], row_count=5))\n",
        "    search_box.change(fn=gradio_live_recommendations, inputs=search_box, outputs=[correction_message, recommendations_dropdown])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "da1ea01a8812413084e8245f1a6642f3",
            "4d4a838b02dd4d01b7d892a8452ef77f",
            "762f0a14da694a578ec0075978acff51",
            "44cc8163562245cf8b7f73a17659bd56",
            "a2037e151dc04056905cd13261e2c1a1",
            "97591235f9a2499592cb5d2ea389a07e",
            "9461eda3c182436c869a34682931ea0a",
            "0f49d79e409646668a66ec2b729f0a95",
            "272cdf648be7435bbd5ef6919ce5ca6b",
            "3ffb720a42184a0e97aa2270f10d161c",
            "214670f5469944b2857c06cabf3a62f8",
            "d33ec049868f406e99c34e1a81988026",
            "198b2a90f1364944bebb315cef90aef9",
            "9431de9cce6a45f7a944e1f94373954e",
            "b8c2355fe14041c7b6158aa2fc395591",
            "31c19c29004e46c29b6ecd60efe343de",
            "319977fc13394235af10a4a33ccbb104",
            "b91a06b97fd74ad592a9b794670ec47b",
            "b8dea14c393840d7add2d43b1c4b4ce9",
            "07b10d2da9c14fa88f1026f9fa9e7a0a",
            "9d8a78109ca446108f868ada646614a7",
            "2c78a598143a42d490fe9b98e4c95665",
            "42c7586651dc4c0c8e6d07946d82626a",
            "92f698a393ec434285ec3bfbf8a03f11",
            "112f7829bb60426786e97e6f737383ab",
            "eb646d871ed346a19c7030cab59c781d",
            "59f4a35b477d423491a35d91fad2db4b",
            "6941f4d7320e44ffb62499a57d4614b8",
            "c42391361b3549678b794bc1a181276e",
            "edf773f3d2da4a1398a7bb38780535e1",
            "11cc5f48598e4cdbac5b77c72ec9ad9d",
            "45478de6528f4eafa5bf7b18fb911213",
            "8c08498782a445b583257d95174d59ad",
            "53024f996a7b48479bdf1a5595aed766",
            "6acc4bd591bd4a2ea807f13a382d99a0",
            "628283585bb44946a6c98d35c6b8392d",
            "62587604ba114e0e9e07df23ee1d4c5f",
            "226feadb30764c419c00850ff06ed60c",
            "2753faf9297549a6befdbf70e71e1239",
            "4209fa6cec264ebc9ebcd733bf916a56",
            "c6eac8cd2dc74b4faae0fd74e362594a",
            "7ebbfe3366db4ba4b218cac39a73d330",
            "0f61e898cc7a480f993349eabed7f5da",
            "ae09a0a91c9646f788cce2af40969816",
            "c5b46d42b3604cd0a23252a6e62f46ae",
            "7f98dec3126948138c605800a3d06e90",
            "def150359fdc4931ab003d37175a66dc",
            "044ea673cabf4928a2553f2d14815568",
            "76ffadd0d5dc4ee0aaed271016ffbc52",
            "641017c8b9924f3ba5c16a0730b5e56f",
            "b152c29dbff74446a2979e760c5010e4",
            "c51a31137c9844b185d0d67dacd70eae",
            "d95bce6c5ae74b159db843fe9219991c",
            "301ae4766d1f47fab3f89c3cca34e7bf",
            "e058858bce9b441bbaba8d950b41e565"
          ]
        },
        "id": "xl-xqSC6ruiY",
        "outputId": "d32ad4af-1423-401d-9947-64f203a90092"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to access https://www.ndtv.com/. Status code: 403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da1ea01a8812413084e8245f1a6642f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d33ec049868f406e99c34e1a81988026"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42c7586651dc4c0c8e6d07946d82626a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53024f996a7b48479bdf1a5595aed766"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5b46d42b3604cd0a23252a6e62f46ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://f2fb9c8f2267ba4c99.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f2fb9c8f2267ba4c99.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://f2fb9c8f2267ba4c99.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPLEMENTING MACHINE LEARNING FOR DOMAIN SPECIFIC QUERIES"
      ],
      "metadata": {
        "id": "ucxTa5W3Q-Eh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_aljazeera():\n",
        "    base_url = \"https://www.aljazeera.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # Example sections to extract\n",
        "        sections = [\"middle-east\", \"africa\",\"asia\",\"europe\",\"asia-pacific\",\"latin-america\",\"us-canada\"]\n",
        "\n",
        "        headlines = []\n",
        "        for section in sections:\n",
        "            section_url = f\"{base_url}{section}/\"\n",
        "            section_response = requests.get(section_url)\n",
        "            if section_response.status_code == 200:\n",
        "                section_soup = BeautifulSoup(section_response.content, \"html.parser\")\n",
        "                articles = section_soup.find_all('article')\n",
        "                for article in articles:\n",
        "                    a_tag = article.find('a')\n",
        "                    if a_tag and 'href' in a_tag.attrs:\n",
        "                        headline = a_tag.get_text(strip=True)\n",
        "                        url = a_tag['href']\n",
        "                        if url.startswith('/'):\n",
        "                            url = base_url + url.lstrip('/')\n",
        "                        headlines.append((headline, section, url))\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "    return headlines\n"
      ],
      "metadata": {
        "id": "__VQKrnvsw_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset():\n",
        "    aljazeera_data = scrape_aljazeera()\n",
        "    # Add similar functions for other news sources if needed\n",
        "\n",
        "    headlines, domains, urls = zip(*aljazeera_data)\n",
        "    return headlines, domains, urls\n",
        "\n",
        "# Create dataset\n",
        "headlines, domains, urls = create_dataset()\n",
        "\n",
        "# Optionally, save to a file for later use\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'headline': headlines,\n",
        "    'domain': domains,\n",
        "    'url': urls\n",
        "})\n",
        "\n",
        "df.to_csv('news_dataset.csv', index=False)\n"
      ],
      "metadata": {
        "id": "O9FHUuTIRcHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import joblib\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('news_dataset.csv')\n",
        "headlines = df['headline'].tolist()\n",
        "domains = df['domain'].tolist()\n",
        "\n",
        "# Create and train the model\n",
        "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
        "model.fit(headlines, domains)\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(model, 'text_classifier.joblib')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTDMa3grRmEA",
        "outputId": "4abacd63-1d2c-45de-a38d-16a0a89a6d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text_classifier.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('news_dataset.csv')\n",
        "headlines = df['headline'].tolist()\n",
        "domains = df['domain'].tolist()\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(headlines, domains, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the model\n",
        "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(model, 'text_classifier.joblib')\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Detailed classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kViDGWGxTZVK",
        "outputId": "0091f1e4-81e9-4ced-fd39-8d54e479537e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "       africa       0.00      0.00      0.00         3\n",
            "         asia       0.33      0.50      0.40         2\n",
            " asia-pacific       0.50      0.50      0.50         2\n",
            "       europe       0.75      1.00      0.86         3\n",
            "latin-america       1.00      0.75      0.86         4\n",
            "  middle-east       1.00      0.25      0.40         4\n",
            "    us-canada       0.33      0.50      0.40         2\n",
            "\n",
            "     accuracy                           0.50        20\n",
            "    macro avg       0.56      0.50      0.49        20\n",
            " weighted avg       0.63      0.50      0.51        20\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.chunk import tree2conlltags\n",
        "from rake_nltk import Rake\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import heapq\n",
        "import gradio as gr\n",
        "from collections import Counter\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Load pre-trained GloVe embeddings\n",
        "glove_vectors = KeyedVectors.load_word2vec_format('glove.6B.50d.txt', binary=False, no_header=True)\n",
        "import joblib\n",
        "\n",
        "# Load the trained model\n",
        "model = joblib.load('text_classifier.joblib')\n",
        "\n",
        "def classify_domain(headline):\n",
        "    prediction = model.predict([headline])\n",
        "    return prediction[0]\n",
        "\n",
        "def scrape_telegraph_india():\n",
        "    base_url = \"https://www.thetelegraphindia.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        headlines = soup.find_all('h2')\n",
        "        urls = []\n",
        "        for h2 in headlines:\n",
        "            a_tag = h2.find('a')\n",
        "            if a_tag and 'href' in a_tag.attrs:\n",
        "                url = a_tag['href']\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://www.thetelegraphindia.com/' + url.lstrip('/')\n",
        "                urls.append((a_tag.get_text(strip=True), url))\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def scrape_bbc():\n",
        "    base_url = \"https://www.aljazeera.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        headlines = soup.find_all('h3')\n",
        "        urls = []\n",
        "        for h3 in headlines:\n",
        "            a_tag = h3.find('a')\n",
        "            if a_tag and 'href' in a_tag.attrs:\n",
        "                url = a_tag['href']\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://www.aljazeera.com/' + url.lstrip('/')\n",
        "                urls.append((a_tag.get_text(strip=True), url))\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def scrape_cnn():\n",
        "    base_url = \"https://edition.cnn.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        headlines = soup.find_all('h3')\n",
        "        urls = []\n",
        "        for h3 in headlines:\n",
        "            a_tag = h3.find('a')\n",
        "            if a_tag and 'href' in a_tag.attrs:\n",
        "                url = a_tag['href']\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://edition.cnn.com' + url.lstrip('/')\n",
        "                urls.append((a_tag.get_text(strip=True), url))\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def extract_news_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        content_paragraphs = soup.find_all('p')\n",
        "        if content_paragraphs:\n",
        "            content = \"\\n\".join(p.get_text(strip=True) for p in content_paragraphs)\n",
        "            return content\n",
        "        return \"Content not found.\"\n",
        "    else:\n",
        "        print(f\"Failed to access {url}. Status code: {response.status_code}\")\n",
        "        return \"Failed to fetch content.\"\n",
        "\n",
        "def extract_entities(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    chunked = ne_chunk(pos_tags, binary=False)\n",
        "    iob_tagged = tree2conlltags(chunked)\n",
        "    entities = [word for word, pos, ner in iob_tagged if ner != 'O']\n",
        "    return entities\n",
        "\n",
        "def extract_keywords(text):\n",
        "    r = Rake()  # Initialize RAKE\n",
        "    r.extract_keywords_from_text(text)\n",
        "    all_keywords = r.get_ranked_phrases()\n",
        "\n",
        "    # Filter to include only 1-word or 2-word phrases\n",
        "    filtered_keywords = [kw for kw in all_keywords if len(kw.split()) <= 2]\n",
        "\n",
        "    # Limit to top 10 keywords\n",
        "    top_keywords = filtered_keywords[:10]\n",
        "\n",
        "    return ', '.join(top_keywords)\n",
        "\n",
        "class NeuralIRSearchEngine:\n",
        "    def __init__(self):\n",
        "        self.news_data, self.news_urls, self.news_domains = self.create_dataset()\n",
        "        self.ngrams = self.create_ngrams(self.news_data)\n",
        "        self.corpus_words = self.create_corpus_words(self.news_data)\n",
        "        self.entities = [extract_entities(headline) for headline in self.news_data]\n",
        "\n",
        "    def create_dataset(self):\n",
        "        aljazeera_data = scrape_aljazeera()\n",
        "        # Add similar functions for other news sources if needed\n",
        "\n",
        "        headlines, domains, urls = zip(*aljazeera_data)\n",
        "        return headlines, urls, domains\n",
        "\n",
        "    def filter_by_domain(self, domain):\n",
        "        filtered_data = [(self.news_data[i], self.news_urls[i], self.news_domains[i])\n",
        "                         for i in range(len(self.news_data)) if self.news_domains[i] == domain]\n",
        "        return filtered_data\n",
        "\n",
        "    def scrape_news(self):\n",
        "        telegraph_data = scrape_telegraph_india()\n",
        "        bbc_data = scrape_bbc()\n",
        "        cnn_data = scrape_cnn()\n",
        "\n",
        "        all_data = telegraph_data + bbc_data + cnn_data\n",
        "        news_data = [headline for headline, url in all_data]\n",
        "        news_urls = [url for headline, url in all_data]\n",
        "        return news_data, news_urls\n",
        "\n",
        "    def create_ngrams(self, data, n=2):\n",
        "        ngrams = []\n",
        "        for text in data:\n",
        "            tokens = word_tokenize(text.lower())\n",
        "            ngrams.extend(list(nltk.ngrams(tokens, n)))\n",
        "        return Counter(ngrams)\n",
        "\n",
        "    def create_corpus_words(self, data):\n",
        "        words = []\n",
        "        for text in data:\n",
        "            tokens = word_tokenize(text.lower())\n",
        "            words.extend(tokens)\n",
        "        return set(words)\n",
        "\n",
        "    def suggest_next_words(self, input_text):\n",
        "        tokens = word_tokenize(input_text.lower())\n",
        "        if len(tokens) < 1:\n",
        "            return []\n",
        "        last_tokens = tuple(tokens[-1:])\n",
        "        possible_ngrams = [ngram for ngram in self.ngrams if ngram[:-1] == last_tokens]\n",
        "        next_word_counts = Counter([ngram[-1] for ngram in possible_ngrams])\n",
        "        suggestions = [word for word, count in next_word_counts.most_common(5)]\n",
        "        return suggestions\n",
        "\n",
        "    def get_average_embedding(self, text):\n",
        "        words = word_tokenize(text.lower())\n",
        "        word_embeddings = []\n",
        "        for word in words:\n",
        "            if word in glove_vectors:\n",
        "                word_embeddings.append(glove_vectors[word])\n",
        "        if word_embeddings:\n",
        "            return np.mean(word_embeddings, axis=0)\n",
        "        else:\n",
        "            return np.zeros(50)\n",
        "\n",
        "    def rank_documents(self, query, documents):\n",
        "        query_embedding = self.get_average_embedding(query)\n",
        "\n",
        "        if np.all(query_embedding == 0):\n",
        "            return [], [], []\n",
        "\n",
        "        doc_embeddings = [self.get_average_embedding(doc) for doc in documents]\n",
        "\n",
        "        if not doc_embeddings:\n",
        "            return [], [], []\n",
        "\n",
        "        doc_embeddings = np.array(doc_embeddings)\n",
        "\n",
        "        similarities = cosine_similarity([query_embedding], doc_embeddings).flatten()\n",
        "        ranked_indices = heapq.nlargest(5, range(len(similarities)), similarities.take)\n",
        "        ranked_documents = [documents[i] for i in ranked_indices]\n",
        "\n",
        "        return ranked_documents, ranked_indices, similarities\n",
        "\n",
        "    def generate_summary(self, text):\n",
        "        return f\"Summary of: {text[:100]}...\"\n",
        "\n",
        "    def search(self, query):\n",
        "        if query:\n",
        "            ranked_news, ranked_indices, similarities = self.rank_documents(query, self.news_data)\n",
        "            summaries = []\n",
        "            for idx in ranked_indices:\n",
        "                full_text = extract_news_from_url(self.news_urls[idx])\n",
        "                summary = self.generate_summary(full_text)\n",
        "                keywords = extract_keywords(full_text)\n",
        "                summaries.append([\n",
        "                    f'{self.news_data[idx]}',\n",
        "                    summary,\n",
        "                    round(similarities[idx], 2),\n",
        "                    keywords,\n",
        "                    \", \".join(self.entities[idx]),\n",
        "                    self.news_urls[idx]  # Adding the URL to the output\n",
        "                ])\n",
        "            return summaries\n",
        "        else:\n",
        "            return \"Please enter a search query.\"\n",
        "\n",
        "    def correct_spelling(self, input_text):\n",
        "        tokens = word_tokenize(input_text.lower())\n",
        "        corrected_tokens = []\n",
        "        for token in tokens:\n",
        "            if token not in self.corpus_words:\n",
        "                # Find closest match using Levenshtein distance\n",
        "                closest_match = min(self.corpus_words, key=lambda word: levenshtein_distance(token, word))\n",
        "                corrected_tokens.append(closest_match)\n",
        "            else:\n",
        "                corrected_tokens.append(token)\n",
        "        return ' '.join(corrected_tokens)\n",
        "\n",
        "search_engine = NeuralIRSearchEngine()\n",
        "\n",
        "def gradio_search(query, selected_domain):\n",
        "    filtered_news = search_engine.filter_by_domain(selected_domain)\n",
        "    if query:\n",
        "        ranked_news, ranked_indices, similarities = search_engine.rank_documents(query, [headline for headline, url, domain in filtered_news])\n",
        "        summaries = []\n",
        "        for idx in ranked_indices:\n",
        "            full_text = extract_news_from_url(filtered_news[idx][1])\n",
        "            summary = search_engine.generate_summary(full_text)\n",
        "            keywords = extract_keywords(full_text)\n",
        "            summaries.append([\n",
        "                f'{filtered_news[idx][0]}',\n",
        "                summary,\n",
        "                round(similarities[idx], 2),\n",
        "                keywords,\n",
        "                \", \".join(search_engine.entities[idx]),\n",
        "                filtered_news[idx][1]\n",
        "            ])\n",
        "        return summaries\n",
        "    else:\n",
        "        return \"Please enter a search query.\"\n",
        "\n",
        "def gradio_live_recommendations(query):\n",
        "    corrected_query = search_engine.correct_spelling(query)\n",
        "    suggestions = search_engine.suggest_next_words(corrected_query)\n",
        "    if corrected_query.lower() != query.lower():\n",
        "        return f\"Did you mean '{corrected_query}'?\", suggestions\n",
        "    else:\n",
        "        return \"\", suggestions\n",
        "df = pd.read_csv('news_dataset.csv')\n",
        "\n",
        "unique_domains = df['domain'].unique().tolist()\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        search_box = gr.Textbox(label=\"Search Query\", placeholder=\"Type here...\", lines=2)\n",
        "        domain_filter = gr.Dropdown(label=\"Domain Filter\", choices=unique_domains)\n",
        "        recommendations_dropdown = gr.Dropdown(label=\"Suggestions\", choices=[], allow_custom_value=True)\n",
        "        correction_message = gr.Textbox(label=\"\", placeholder=\"Go on and ask your query\", lines=1)\n",
        "        submit_button = gr.Button(\"Search\")\n",
        "\n",
        "    submit_button.click(fn=gradio_search, inputs=[search_box, domain_filter], outputs=gr.Dataframe(headers=[\"Headline\", \"Summary\", \"Relevance Score\", \"Highlights\", \"Keywords\", \"URL\"], row_count=5))\n",
        "    search_box.change(fn=gradio_live_recommendations, inputs=search_box, outputs=[correction_message, recommendations_dropdown])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "BP-XDgD0RqWO",
        "outputId": "7c39a268-0732-49bf-8664-4051e49c2895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://8689ec10f5777dd338.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8689ec10f5777dd338.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://8689ec10f5777dd338.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For faster running and skipping the summarisation pipeline run this"
      ],
      "metadata": {
        "id": "u8TfaQCLWiM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from transformers import pipeline\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.chunk import tree2conlltags\n",
        "from rake_nltk import Rake\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import heapq\n",
        "import gradio as gr\n",
        "from collections import Counter\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "custom_css = \"\"\"\n",
        ".gradio-container {\n",
        "    background: url(https://res.cloudinary.com/dye74kvqk/image/upload/v1726165064/5b5f024f-39cd-4c91-8973-8515968b68d9.png);\n",
        "    background-size: cover;\n",
        "    background-position: center;\n",
        "}\n",
        "\n",
        ".gradio-container h1 {\n",
        "    text-align: center;\n",
        "    color: white;\n",
        "    font-size: 50px;\n",
        "    margin-left:70%\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Load pre-trained GloVe embeddings\n",
        "glove_vectors = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/nlp/glove.6B.50d.txt', binary=False, no_header=True)\n",
        "\n",
        "#Scraping process begins - we can add more websites to increase the reach\n",
        "def scrape_telegraph_india():\n",
        "    base_url = \"https://www.ndtv.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        headlines = soup.find_all('h2')\n",
        "        urls = []\n",
        "        for h2 in headlines:\n",
        "            a_tag = h2.find('a')\n",
        "            if a_tag and 'href' in a_tag.attrs:\n",
        "                url = a_tag['href']\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://www.ndtv.com/' + url.lstrip('/')\n",
        "                urls.append((a_tag.get_text(strip=True), url))\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def scrape_bbc():\n",
        "    base_url = \"https://www.aljazeera.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        headlines = soup.find_all('h3')\n",
        "        urls = []\n",
        "        for h3 in headlines:\n",
        "            a_tag = h3.find('a')\n",
        "            if a_tag and 'href' in a_tag.attrs:\n",
        "                url = a_tag['href']\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://www.aljazeera.com/' + url.lstrip('/')\n",
        "                urls.append((a_tag.get_text(strip=True), url))\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "def scrape_cnn():\n",
        "    base_url = \"https://www.cnn.com/\"\n",
        "    response = requests.get(base_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        # Find all span elements with the specified class\n",
        "        headline_spans = soup.find_all('span', class_='container__headline-text')\n",
        "        urls = []\n",
        "        for span in headline_spans:\n",
        "            headline_text = span.get_text(strip=True)\n",
        "            # Find the parent <a> tag for the URL\n",
        "            parent_a_tag = span.find_parent('a')\n",
        "            if parent_a_tag and 'href' in parent_a_tag.attrs:\n",
        "                url = parent_a_tag['href']\n",
        "                if url.startswith('/'):\n",
        "                    url = 'https://www.cnn.com/' + url.lstrip('/')\n",
        "                urls.append((headline_text, url))\n",
        "        return urls\n",
        "    else:\n",
        "        print(f\"Failed to access {base_url}. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "# Here we are extracting the content from the news.\n",
        "def extract_news_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        content_paragraphs = soup.find_all('p')\n",
        "        if content_paragraphs:\n",
        "            content = \"\\n\".join(p.get_text(strip=True) for p in content_paragraphs)\n",
        "            return content\n",
        "        return \"Content not found.\"\n",
        "    else:\n",
        "        print(f\"Failed to access {url}. Status code: {response.status_code}\")\n",
        "        return \"Failed to fetch content.\"\n",
        "\n",
        "# Now extract places names organisations to highlight them in the headlines\n",
        "def extract_entities(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    chunked = ne_chunk(pos_tags, binary=False)\n",
        "    iob_tagged = tree2conlltags(chunked)\n",
        "    entities = [word for word, pos, ner in iob_tagged if ner != 'O']\n",
        "    return entities\n",
        "\n",
        "# Using Rake to get keywords\n",
        "def extract_keywords(text):\n",
        "    r = Rake()  # Initialize RAKE\n",
        "    r.extract_keywords_from_text(text)\n",
        "    all_keywords = r.get_ranked_phrases()\n",
        "\n",
        "    filtered_keywords = [kw for kw in all_keywords if len(kw.split()) <= 2]\n",
        "\n",
        "    top_keywords = filtered_keywords[:10]\n",
        "\n",
        "    return ', '.join(top_keywords)\n",
        "\n",
        "# Main code begins  where we implement information retrieval\n",
        "class NeuralIRSearchEngine:\n",
        "    def __init__(self):\n",
        "        self.news_data, self.news_urls = self.scrape_news()\n",
        "        self.ngrams = self.create_ngrams(self.news_data)\n",
        "        self.corpus_words = self.create_corpus_words(self.news_data)\n",
        "        self.entities = [extract_entities(headline) for headline in self.news_data]\n",
        "        # self.summarizer = pipeline(\"summarization\")\n",
        "\n",
        "    def scrape_news(self):\n",
        "        telegraph_data = scrape_telegraph_india()\n",
        "        bbc_data = scrape_bbc()\n",
        "        cnn_data = scrape_cnn()\n",
        "\n",
        "        all_data = telegraph_data + bbc_data + cnn_data\n",
        "        news_data = [headline for headline, url in all_data]\n",
        "        news_urls = [url for headline, url in all_data]\n",
        "        return news_data, news_urls\n",
        "\n",
        "    def create_ngrams(self, data, n=2):\n",
        "        ngrams = []\n",
        "        for text in data:\n",
        "            tokens = word_tokenize(text.lower())\n",
        "            ngrams.extend(list(nltk.ngrams(tokens, n)))\n",
        "        return Counter(ngrams)\n",
        "\n",
        "    def create_corpus_words(self, data):\n",
        "        words = []\n",
        "        for text in data:\n",
        "            tokens = word_tokenize(text.lower())\n",
        "            words.extend(tokens)\n",
        "        return set(words)\n",
        "\n",
        "    def suggest_next_words(self, input_text):\n",
        "        tokens = word_tokenize(input_text.lower())\n",
        "        if len(tokens) < 1:\n",
        "            return []\n",
        "        last_tokens = tuple(tokens[-1:])\n",
        "        possible_ngrams = [ngram for ngram in self.ngrams if ngram[:-1] == last_tokens]\n",
        "        next_word_counts = Counter([ngram[-1] for ngram in possible_ngrams])\n",
        "        suggestions = [word for word, count in next_word_counts.most_common(5)]\n",
        "        return suggestions\n",
        "\n",
        "    def get_average_embedding(self, text):\n",
        "        words = word_tokenize(text.lower())\n",
        "        word_embeddings = []\n",
        "        for word in words:\n",
        "            if word in glove_vectors:\n",
        "                word_embeddings.append(glove_vectors[word])\n",
        "        if word_embeddings:\n",
        "            return np.mean(word_embeddings, axis=0)\n",
        "        else:\n",
        "            return np.zeros(50)\n",
        "\n",
        "    def rank_documents(self, query, documents):\n",
        "        query_embedding = self.get_average_embedding(query)\n",
        "\n",
        "        if np.all(query_embedding == 0):\n",
        "            return [], [], []\n",
        "\n",
        "        doc_embeddings = [self.get_average_embedding(doc) for doc in documents]\n",
        "\n",
        "        if not doc_embeddings:\n",
        "            return [], [], []\n",
        "\n",
        "        doc_embeddings = np.array(doc_embeddings)\n",
        "\n",
        "        similarities = cosine_similarity([query_embedding], doc_embeddings).flatten()\n",
        "        ranked_indices = heapq.nlargest(5, range(len(similarities)), similarities.take)\n",
        "        ranked_documents = [documents[i] for i in ranked_indices]\n",
        "\n",
        "        return ranked_documents, ranked_indices, similarities\n",
        "\n",
        "    def generate_summary(self, text):\n",
        "        return f\"{text[:100]}...\"\n",
        "        # try:\n",
        "        #     max_input_length = 1024\n",
        "        #     input_tokens = self.summarizer.tokenizer(text, return_tensors='pt', truncation=True, max_length=max_input_length)\n",
        "        #     input_text = self.summarizer.tokenizer.decode(input_tokens['input_ids'][0], skip_special_tokens=True)\n",
        "\n",
        "        #     summary = self.summarizer(input_text, max_length=150, min_length=50, do_sample=False)[0]['summary_text']\n",
        "        #     return summary\n",
        "        # except Exception as e:\n",
        "        #     print(f\"Error during summarization: {e}\")\n",
        "        #     return \"Summary could not be generated.\"\n",
        "\n",
        "    def search(self, query):\n",
        "        if query:\n",
        "            ranked_news, ranked_indices, similarities = self.rank_documents(query, self.news_data)\n",
        "            summaries = []\n",
        "            for idx in ranked_indices:\n",
        "                full_text = extract_news_from_url(self.news_urls[idx])\n",
        "                summary = self.generate_summary(full_text)\n",
        "                keywords = extract_keywords(full_text)\n",
        "                summaries.append([\n",
        "                    f'{self.news_data[idx]}',\n",
        "                    summary,\n",
        "                    round(similarities[idx], 2),\n",
        "                    keywords,\n",
        "                    \"-- \".join(self.entities[idx]),\n",
        "                    self.news_urls[idx]  # Adding the URL to the output\n",
        "                ])\n",
        "            return summaries\n",
        "        else:\n",
        "            return \"Please enter a search query.\"\n",
        "\n",
        "    def correct_spelling(self, input_text):\n",
        "        tokens = word_tokenize(input_text.lower())\n",
        "        corrected_tokens = []\n",
        "        for token in tokens:\n",
        "            if token not in self.corpus_words:\n",
        "                # Find closest match using Levenshtein distance\n",
        "                closest_match = min(self.corpus_words, key=lambda word: levenshtein_distance(token, word))\n",
        "                corrected_tokens.append(closest_match)\n",
        "            else:\n",
        "                corrected_tokens.append(token)\n",
        "        return ' '.join(corrected_tokens)\n",
        "\n",
        "search_engine = NeuralIRSearchEngine()\n",
        "\n",
        "def gradio_search(query):\n",
        "    results = search_engine.search(query)\n",
        "    if isinstance(results, str):\n",
        "        return results\n",
        "    return results\n",
        "\n",
        "def gradio_live_recommendations(query):\n",
        "    corrected_query = search_engine.correct_spelling(query)\n",
        "    suggestions = search_engine.suggest_next_words(corrected_query)\n",
        "    if corrected_query.lower() != query.lower():\n",
        "        return f\"Did you mean '{corrected_query}'?\", suggestions\n",
        "    else:\n",
        "        return \"\", suggestions\n",
        "\n",
        "title_html = \"<h1>Super Nova</h1>\"\n",
        "\n",
        "with gr.Blocks(css=custom_css) as demo:\n",
        "    gr.HTML(title_html)\n",
        "\n",
        "    with gr.Row():\n",
        "        search_box = gr.Textbox(label=\"Search Query\", placeholder=\"Type here...\", lines=2)\n",
        "        recommendations_dropdown = gr.Dropdown(label=\"Suggestions\", choices=[], allow_custom_value=True)\n",
        "        correction_message = gr.Textbox(label=\"\", placeholder=\"Go on and ask your query\", lines=1)\n",
        "        submit_button = gr.Button(\"Search\")\n",
        "\n",
        "    submit_button.click(fn=gradio_search, inputs=search_box, outputs=gr.Dataframe(headers=[\"Headline\", \"Summary\", \"Relevance Score\", \"Highlights\", \"Keywords\", \"URL\"], row_count=5))\n",
        "    search_box.change(fn=gradio_live_recommendations, inputs=search_box, outputs=[correction_message, recommendations_dropdown])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "PY5gVnUFVuDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M_7BbJ58Yl1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cK1h6FMBWUre"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}